<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>blender journey</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="sidebar">
        <h1>503A, design blog</h1>
        <nav>
            <a href="#take1" class="nav-link active">take 1, music notes</a>
            <a href="#take2" class="nav-link">take 2, starting the interface</a>
            <a href="#take3" class="nav-link">take 3, final renders + notion</a>
            <a href="#take4" class="nav-link">take 4, reading response</a>
            <a href="#take5" class="nav-link">take 5, Pattern, rhythm, movement</a>
            <a href="#take6" class="nav-link">take 6, Time and oscillation</a>
            <a href="#take7" class="nav-link">take 7, Find your interface</a>
            <a href="#take8" class="nav-link">take 8, refining my interface</a>
        </nav>
    </div>

    <div class="container">
        <main>
            <article class="blog-post" id="take1">
                <h2>TAKE 1, music notes</h2>
                <div class="post-meta">
                    <time datetime="2025-01-27">OCTOBER 14 2025</time>
                </div>

                <div class="post-content">
                    <p>
                        <!-- space for your writing -->
                        picking up a new (but really old) software in 2025 always feels taunting, especially when your laziness can't depend on an AI chat assistant to guide you through the difficulties of a new UI environment. I've known of Blender for years now. From a distance, it was always something that I would just lend my respect towards, even more so to the souls that could decipher it’s uses. For this training-wheels period, I took on modeling music notes from the circle of fifths chart that lives pasted at the top of my workspace in the living room of my KTown kingdom. 
                    </p>

                    <div class="image-block">
                        <img src="blender-screens/s1.png" alt="blender screenshot 1">
                        <p class="image-caption">angle 1: my shapes, lights, and a camera set up</p>
                    </div>

                    <p>
                        <!-- space for your writing -->
                        Ideation was simple. I make music and do all my design projects from the same standings desk, so I tend to mix the two mediums at any given opportunity. Truthfully, I felt like at this point in my 3d career, music notes wouldn’t have me pulling my hair out– a sight my lovely roommates wouldn’t want to see. I present the beauty of a half note, a quarter note, and an eighth note. 
                    </p>

                    <div class="image-block">
                        <img src="blender-screens/s2.png" alt="blender screenshot 2">
                        <p class="image-caption">angle 2</p>
                    </div>

                    <p>
                        <!-- space for your writing -->
                        To construct this masterpiece, I relied heavily on the use of UV spheres and a single square, for the eighth note’s flag. After making the quarter note first to establish a template, i was able to have a foundation for the rest of the notes construction– even hollowing out the center of the half note!
                    </p>

                    <div class="image-block">
                        <img src="blender-screens/s3.png" alt="blender screenshot 3">
                        <p class="image-caption">angle 3</p>
                    </p>

                    <p>
                        To be honest, the joy in coloring, adding light, and the floor/background provided me enough pleasure to want to come back to this software. Usually with things like this, i try to look for specs of satisfaction that i can strive to fulfill on each go – i’m fully aware this won’t be a consistent feeling though, especially as the project scopes grow beyond just a couple music notes
                    </p>

                    <div class="image-block">
                        <img src="final-render/3-notes-renders.png" alt="final render">
                        <p class="image-caption">final render - 3 notes</p>
                    </div>
                </div>
            </article>

            <article class="blog-post" id="take2">
                <h2>take 2, starting the interface</h2>
                <div class="post-meta">
                    <time datetime="2025-01-27">october 21 2025</time>
                </div>

                <div class="post-content">
                    <p>
                        interfaces are my comfort – when the process began of experimenting with interface in blender, i knew there was an opportunity for me to establish further comfort in the software. to reiterate, i couldn't be more of a newbie to 3d modeling, it scares me…for this assignment though, i entered the world of a design freelancer, locked in their studio dungeon – often times in the dark – and the brief set up that allows them dissociate from the real world. haha, just jokes, mostly at myself.
                    </p>

                    <div class="image-block">
                        <img src="take-2-images/image-render-interface-5.png" alt="blender interface screenshot 5">
                        <p class="image-caption">Notion: my everyday productivity tool, the focus of the dashboard itself</p>
                    </div>

                    <p>
                        notion is a tool that i've used since early undergrad despite the many frustrations that i've developed with it. it's not because its a bad experience, i just crave more! with this interface, i wanted a freelancer, a designer perhaps, to have the opportunity to engage with a client who is new to notion, and allow them to communicate seamlessly on their collaborative efforts using a variety of visual indicators. of course, i'd be building within notion's popular database system.
                    </p>

                    <div class="image-block">
                        <img src="take-2-images/image-render-interface-1.png" alt="blender interface screenshot 1">
                        <p class="image-caption">pre-texture scene</p>
                    </div>

                    <p>
                        as per the assignment guidelines, i started with the room that this interface would be built in – the desk, the chairs, the speakers, the laptop, the monitors– all the basics which every async worker tends to have. in comparison to last week where i was just getting a feel for things, i was able to experiment more with materials, colors, and textures to replicate my own work-space – the place i write this blog post from.
                    </p>

                    <div class="image-block">
                        <img src="take-2-images/image-render-interface-2.png" alt="blender interface screenshot 2">
                        <p class="image-caption">messing around with materials, the wood from my studio space!</p>
                    </div>

                    <p>
                        in looking at my renders, i definitely would like to learn more about lighting and creating an outside world from within. to add, the windows that you see in my scene (yes, those are windows -__-) don't really have a depth or view to anything beyond the endless canvas. i'd either like to add an image or create some assets to make the room feel more accurate to how it really is.
                    </p>

                    <div class="image-block">
                        <img src="take-2-images/image-render-interface-3.png" alt="blender interface screenshot 3">
                        <p class="image-caption">windows!</p>
                    </div>

                    <p>
                        in terms of third party resources, i did use blenderkit to source the accurate tech that i needed for my scene – i felt like my skills at this point wouldn't have allowed me to accurately replicate the materials i use on the day to day, so i decided this route worked best. all in all, i look forward to adding the actual interface to my environment soon!
                    </p>

                    <div class="image-block">
                        <img src="take-2-images/image-render-interface-4.png" alt="final render">
                        <p class="image-caption">final render, a dark, moody work space</p>
                    </div>
                </div>
            </article>

            <article class="blog-post" id="take3">
                <h2>take 3, final renders + notion interface</h2>
                <div class="post-meta">
                    <time datetime="2025-10-28">october 28 2025</time>
                </div>

                <div class="post-content">
                    <p>
                        final renderings with my notion dashboard visible on the monitor—kept the dark studio mood while making the interface readable.
                    </p>

                    <div class="image-block">
                        <img src="take-3-images/t3-1.png" alt="final render 1">
                        <p class="image-caption">final render, angle 1</p>
                    </div>

                    <div class="image-block">
                        <img src="take-3-images/t3-2.png" alt="final render 2">
                        <p class="image-caption">final render, angle 2</p>
                    </div>
                </div>
            </article>

            <article class="blog-post" id="take4">
                <h2>take 4, reading response</h2>
                <div class="post-meta">
                    <time datetime="2024-11-04">November 4 2025</time>
                </div>

                <div class="post-content">
                    <h3>Reading 1, Programming Design Systems</h3>

                    <blockquote>
                        <p>"Is google.com a good website because of the look of the search field?"</p>
                    </blockquote>

                    <p>
                        This sentence is set up by the prior one, which describes how limiting it is to define design and just the style of the page, a particularly interesting token that all UX institutions think about all day. With digital interfaces – the ones built with code – so much in psychology is required to activate the terms "good design" in regards to an app or website. The google search field, which is basic aesthetic, powers billions of searches a day – tell me how that isn't good?
                    </p>

                    <blockquote>
                        <p>"Code allows designers to not just create designs, but build digital systems that create designs."</p>
                    </blockquote>

                    <p>
                        In the era of vibe-code, a quote like this is far less daunting than it once was. The ever consuming nature of programming, in my brain at least, resembles that of the world and inevitably a creator.
                    </p>

                    <blockquote>
                        <p>"Imagine a small village in Japan with its muted, desaturated colors, and compare this to a place like Mexico, where houses are painted in very pure, saturated colors. These colors reflect the culture around them, and you should consider your content in the same way: Does it demand lively colors or a subdued, modernist scheme? The saturation of your colors is the key to this."</p>
                    </blockquote>

                    <p>
                        After many years of seeing the "Mexican filter" convos on twitter, it's great to see cultures like my own contextualized more properly and how color plays a role in the richness of it. My favorite part of traveling is actually the color inspo I pick up from regions who practice different traditions or combinations!
                    </p>

                    <h3>Reading 2, What do Prototypes Prototype?</h3>

                    <blockquote>
                        <p>"By focusing on the purpose of the prototype—that is, on what it prototypes—we can make better decisions about the kinds of prototypes to build."</p>
                    </blockquote>

                    <p>
                        I am the biggest culprit here – as someone who tends to lollygag around the low to mid-fi stages of prototyping, I'd be lying if I said it was out of intention. Thinking about why these stages exist in the design process is crucial to successfully meeting the goal at hand.
                    </p>

                    <blockquote>
                        <p>"Finished-looking (or -behaving) prototypes are often thought to indicate that the design they represent is near completion… It is important to recognize that the degree of visual and behavioral refinement of a prototype does not necessarily correspond to the solidity of the design."</p>
                    </blockquote>

                    <p>
                        Aesthetic achievement isn't a guarantee in digital products! Actions can be misleading, the flow can be out of sorts – a whole list of boxes need to be checked from the users side before solidity can be confirmed.
                    </p>

                    <blockquote>
                        <p>"It was more efficient to wait on the results of independent investigations in the key areas of role, look and feel, and implementation than to try to build a monolithic prototype that integrated all features from the start."</p>
                    </blockquote>

                    <p>
                        Truth nuke. Sometimes it feels like going back to fix issues takes longer (and tbh is way less fun) than just being patient with the research stage to inform the MVP.
                    </p>
                </div>
            </article>

            <article class="blog-post" id="take5">
                <h2>take 5, Pattern, rhythm, movement</h2>
                <div class="post-meta">
                    <time datetime="2025-11-10">November 10 2025</time>
                </div>

                <div class="post-content">
                    <p>
                        i encountered many logic, gravity-filled issues throughout the process to inform the final the solution i arrived to. the goal was to add animation to express movement, specifically complex, non-linear movement as per the assignment guidelines.                     </p>

                    <div class="image-block">
                        <img src="take-5/process-1.png" alt="process 1">
                        <p class="image-caption">process 1</p>
                    </div>

                    <div class="image-block">
                        <img src="take-5/process-2.png" alt="process 2">
                        <p class="image-caption">process 2</p>
                    </div>

                    <h3>Problem: Finding Space in a Dynamic Pool</h3>

                    <p>
                        One of the most interesting challenges in this project was implementing the logic for dropping an album back into the pool and having it naturally find space among the other albums through gravity and physics-based interactions. The initial approach attempted to pre-calculate a target position by selecting a random slot among evenly-spaced album positions. However, this deterministic approach failed because it didn't account for the dynamic, non-uniform arrangement of albums that had already been dropped and settled. The solution was to abandon pre-calculated positioning entirely and instead implement a physics-based system where the falling album naturally finds its place through gravitational forces, collision detection, and reactive interactions with existing albums.
                    </p>


                    <div class="image-block">
                        <a href="https://buribe13.github.io/pattern-rhythm-movement/" target="_blank" rel="noopener noreferrer">
                            <img src="take-5/final-result-3.png" alt="final result">
                        </a>
                        <p class="image-caption">final result, click image to see project -></p>
                    </div>
                    <p>
                        The system works through a multi-layered physics simulation. Gravity is applied continuously, and the album's position updates each frame based on its velocity. When the album reaches the bottom boundary, it bounces with energy damping, gradually losing momentum until it settles. The critical innovation is the collision detection loop that checks the falling album against every other album in the pool. When a collision is detected, the system calculates the overlap distance and collision angle, then separates the albums by pushing them apart along the collision normal. The falling album's velocity is reflected using a dot product calculation, creating realistic bouncing behavior. Simultaneously, existing albums reactively push away from the falling album when it gets within a reaction distance, creating space proactively rather than waiting for collisions. This dual approach, reactive pushing and collision-based bouncing, ensures the falling album naturally carves out space in the pool. The album settles when its total velocity drops below a threshold and it's resting on the bottom, at which point its final position is preserved, allowing the pool to maintain its organic, physics-driven arrangement rather than forcing a uniform grid layout.
                    </p>
                </div>
            </article>

            <article class="blog-post" id="take6">
                <h2>take 6, Time and oscillation</h2>
                <div class="post-meta">
                    <time datetime="2025-11-24">November 24 2025</time>
                </div>

                <div class="post-content">
                    <p>
                        This project explores the intersection of time-based animation and organic motion through interactive 3D graphics. Using p5.js, I created two distinct shapes with different movement patterns: one with continuous oscillating motion and another with interval-based transformations, all while incorporating Perlin noise to achieve organic, flowing forms.
                    </p>

                    <h3>Iteration 1: Static 3D Fibonacci Sphere</h3>
                    <p>
                        The foundation began with a static Fibonacci sphere—a mathematically elegant distribution of 1000 points across a spherical surface using the golden ratio and golden angle. This structure provides even point distribution without clustering at the poles, creating a perfectly balanced geometric form. At this stage, the sphere remains motionless, serving as a canvas to understand the underlying mathematical structure before introducing movement.
                    </p>

                    <div class="image-block">
                        <img src="take-6-Time-and-oscillation/static.png" alt="initial static state">
                        <p class="image-caption">initial static state</p>
                    </div>

                    <h3>Iteration 2: Adding Rotation Animation</h3>
                    <p>
                        In the second iteration, I introduced continuous rotation using trigonometric functions. The sphere now rotates on both the Y-axis and X-axis (at half the speed), creating a compound rotation that reveals the three-dimensional nature of the form. The rotation uses rotateY(rotation) and rotateX(rotation * 0.5), with the rotation value incrementing by 0.005 each frame. This creates the repeated, cyclical motion required by the assignment—a perpetual movement that never stops or changes rhythm.
                    </p>

                    <div class="image-block">
                        <video controls width="100%" preload="metadata" playsinline>
                            <source src="take-6-Time-and-oscillation/2-animation.mov" type="video/quicktime">
                            <source src="take-6-Time-and-oscillation/2-animation.mov" type="video/mp4">
                            <a href="take-6-Time-and-oscillation/2-animation.mov">Download video</a> if it doesn't play.
                        </video>
                        <p class="image-caption">process iteration 2 - adding animation</p>
                    </div>

                    <h3>Iteration 3: Adding Speed Control</h3>
                    <p>
                        The third iteration introduced user control over the animation speed through an interactive slider. This allows real-time manipulation of the rotation rate from 0 (completely static) to 10 (very fast). The speed parameter multiplies the rotation increment, giving viewers the ability to slow down and appreciate the geometric structure or speed up to see the motion blur. This control panel uses React state management to pass the speed value into the p5.js draw loop without causing re-renders.
                    </p>

                    <div class="image-block">
                        <video controls width="100%" preload="metadata" playsinline>
                            <source src="take-6-Time-and-oscillation/3-speed.mov" type="video/quicktime">
                            <source src="take-6-Time-and-oscillation/3-speed.mov" type="video/mp4">
                            <a href="take-6-Time-and-oscillation/3-speed.mov">Download video</a> if it doesn't play.
                        </video>
                        <p class="image-caption">process iteration 3 - adjusting speed</p>
                    </div>

                    <h3>Iteration 4: Adding Wobble and Size Controls</h3>
                    <p>
                        Here, the project takes a significant leap by introducing Perlin noise to create organic, breathing movement. The "wobble" parameter uses p.noise() to modulate the radius of each point on the sphere, creating a living, pulsating form that feels biological rather than purely geometric. The noise function samples from a 3D noise space using the point's position and accumulated time, ensuring smooth, continuous deformation. Additionally, a size control was added to scale the entire form, and the wobble can range from 0 (perfect sphere) to 100 (highly distorted organic blob).
                    </p>

                    <div class="image-block">
                        <video controls width="100%" preload="metadata" playsinline>
                            <source src="take-6-Time-and-oscillation/4-multiple.mov" type="video/quicktime">
                            <source src="take-6-Time-and-oscillation/4-multiple.mov" type="video/mp4">
                            <a href="take-6-Time-and-oscillation/4-multiple.mov">Download video</a> if it doesn't play.
                        </video>
                        <p class="image-caption">process iteration 4 - adding multiple elements</p>
                    </div>

                    <h3>Iteration 5: Full Interactive Experience with Two Shapes</h3>
                    <p>
                        The final iteration fulfills all assignment requirements by introducing a second shape with distinct movement characteristics:
                    </p>

                    <p>
                        <strong>Shape 1: Fibonacci Sphere (Continuous Motion)</strong><br>
                        Uses trigonometric rotation for repeated, cyclical movement. Incorporates Perlin noise for organic wobbling. Moves continuously and smoothly with no interruptions.
                    </p>

                    <p>
                        <strong>Shape 2: Organic Torus Rings (Interval Motion)</strong><br>
                        Features time-based interval changes every 2 seconds using p.millis() and p.random(). When the 2-second interval passes, a new random offset is generated, causing sudden shifts in the form. Multiple concentric rings create a torus-like structure with sine-wave modulated radii. Also incorporates Perlin noise for organic undulation.
                    </p>

                    <p>
                        <strong>De-synchronized Motion</strong><br>
                        The two shapes move at completely different rhythms: Shape 1 has smooth, continuous oscillation, while Shape 2 has periodic, interval-based jumps every 2 seconds. Both can rotate at controllable speeds, but their internal motion patterns remain distinct.
                    </p>

                    <p>
                        <strong>Interactive Controls</strong><br>
                        The control panel offers five parameters: Mode Toggle (switch between Shape 1 and Shape 2), Points (control the density of the geometric structure, 10-2000 points), Speed (control rotation speed, 0-10), Wobble (control the intensity of Perlin noise distortion, 0-100), Size (scale the entire form, 0.1-3x), and Trails (create motion blur effects for a more organic, ethereal appearance, 0-255).
                    </p>

                    <div class="image-block">
                        <img src="take-6-Time-and-oscillation/fin-1.jpeg" alt="final result 1">
                        <p class="image-caption">final result 1</p>
                    </div>

                    <div class="image-block">
                        <img src="take-6-Time-and-oscillation/fin-2.jpeg" alt="final result 2">
                        <p class="image-caption">final result 2</p>
                    </div>

                    <h3>Technical Implementation</h3>
                    <p>
                        The project uses p5.js in WEBGL mode for 3D rendering, trigonometric functions (Math.sin, Math.cos, rotateY, rotateX) for repeated motion, time functions (p.millis(), deltaTime) for interval-based movements, Perlin noise (p.noise()) for organic shape distortion, React + TypeScript for the interface and state management, and Next.js for the web framework. The animations are de-synchronized through different timing mechanisms: Shape 1 uses frame-by-frame incremental rotation, while Shape 2 uses real-time clock checks for discrete interval changes.
                    </p>

                    <h3>
                        <a href="https://time-and-oscillation-6g4fvjcqo-benjamin-uribes-projects.vercel.app/" target="_blank" rel="noopener noreferrer">
                            Final Result →
                        </a>
                    </h3>
                </div>
            </article>

            <article class="blog-post" id="take7">
                <h2>take 7, Find your interface</h2>
                <div class="post-meta">
                    <time datetime="2025-12-09">DECEMBER 9 2025</time>
                </div>

                <div class="post-content">
                    <p>
                        exploring gesture-based interactions has always felt like the natural next step after spending so much time in the world of mouse clicks and keyboard shortcuts. there's something inherently more human about using your hands to communicate with a digital interface – it bridges that gap between the physical and the virtual in a way that feels almost instinctual.
                    </p>

                    <p>
                        for this project, i wanted to create something that felt playful and immediate. a photobooth that responds to gestures rather than button presses. the concept itself isn't revolutionary, but the challenge was making it feel natural and responsive – like the interface was actually reading your intentions rather than just detecting hand positions.
                    </p>

                    <p>
                        the core interaction is simple: pinch to take a photo. but building that recognition system required thinking through edge cases i hadn't considered before. when does a pinch actually register as intentional? how do you prevent accidental triggers? what happens when someone's hand drifts out of frame mid-gesture? these questions became the foundation of the entire experience.
                    </p>

                    <p>
                        i implemented two modes – normal and strip – each with their own personality. the strip mode especially felt like a throwback to those photo booth strips you'd get at arcades or malls, where the physical artifact was part of the charm. translating that into a digital space meant thinking about how the layout and timing could recreate that same sense of anticipation and collection.
                    </p>

                    <p>
                        the timer feature adds another layer of intentionality. it's not just about capturing a moment, but about preparing for it. that countdown creates a small ritual around the photo-taking process, which i think makes the whole experience feel more considered and less disposable.
                    </p>

                    <p>
                        effects were the fun part – the layer where i could experiment with visual transformations that felt responsive to the gesture itself. the challenge was making sure they enhanced the moment rather than overwhelming it. sometimes restraint is the hardest design decision to make.
                    </p>

                    <p>
                        what i'm most curious about now is how this kind of interaction scales. can gesture-based interfaces feel as reliable as traditional ones? or are they destined to remain in the realm of playful experiments? either way, building this photobooth reminded me why i got into interface design in the first place – that moment when technology feels less like a barrier and more like an extension of how we naturally want to interact with the world.
                    </p>

                    <p>
                        you can explore the <a href="https://buribe13.github.io/find-your-interface/prototype/" target="_blank" rel="noopener noreferrer">gesture photobooth prototype</a> and see how it responds to your movements. pinch to capture, experiment with the modes, and let me know if the interaction feels as natural as i hoped it would.
                    </p>
                </div>
            </article>

            <article class="blog-post" id="take8">
                <h2>take 8, refining my interface</h2>
                <div class="post-meta">
                    <time datetime="2025-12-16">DECEMBER 16 2025</time>
                </div>

                <div class="post-content">
                    <h3>Retro Photo Booth: Building a High-Fidelity Prototype That You Can Control With Your Body</h3>
                    
                    <p>
                        I set out to build a high-fidelity prototype that feels like a real product, not a mockup: a retro macOS-inspired Photo Booth that you can actually use. The core challenge was balancing three things at once: <strong>look and feel</strong>, <strong>technical implementation</strong>, and <strong>role in life</strong>. The result is an interactive Photo Booth web app that runs in the browser, uses a webcam for real-time video, and treats the user’s body as an input device—<strong>pinch to drag the window</strong> and <strong>smile-with-teeth to trigger the shutter</strong>—with normal mouse/keyboard controls as fallbacks.
                    </p>

                    <p>
                        At a glance, the project is a reinterpretation of a familiar tool. Photo Booth is already a cultural object: it’s playful, a little performative, and designed for “moments.” I wanted to preserve that nostalgia while reframing the interaction model. Instead of needing a mouse click at exactly the right time, the system watches for the moment you’re <em>actually ready</em>—the smile—and captures then. Instead of dragging a window with a cursor, you can “grab” the title bar with a pinch gesture and move it around like a physical object on your desktop.
                    </p>

                    <div class="image-block">
                        <img src="take%208%20images/final-image.jpeg" alt="Final retro photo booth interface">
                        <p class="image-caption">the final result: a functional, retro-styled photo booth</p>
                    </div>

                    <h3>Role in Life: “Body as Controller” as a More Natural Interface</h3>

                    <p>
                        The role-in-life direction for this prototype is simple: <strong>make digital interaction feel closer to everyday physical intention</strong>. In real life, when you take a photo, you don’t think “press the red button now”—you pose, adjust, laugh, and smile. The camera shutter is most meaningful when it responds at the moment you’re expressing what you want to capture. That’s why the primary interaction is <strong>smile-with-teeth capture</strong>.
                    </p>

                    <p>
                        The other embodied interaction is <strong>pinch-to-drag</strong>. A pinch gesture is widely understood as “grab.” Mapping that to moving a window is an intuitive bridge between physical action and UI behavior: it’s not just a novelty input—it matches how the interface already behaves conceptually (a window as a movable object).
                    </p>
                    
                    <p>
                        Because webcam-based embodied input can be messy (lighting, jitter, false positives), I treated “role in life” as not only a concept but also a responsibility: the system needs to be understandable and forgiving. That’s why the prototype includes visible status feedback (“hand tracking,” “face tracking,” smile indicator) and fallback controls (mouse drag, click capture, spacebar).
                    </p>

                    <h3>Look & Feel: High Fidelity as an Experience, Not a Skin</h3>

                    <p>
                        The “look and feel” goal wasn’t just to make it pretty—it was to make it believable. The desktop environment uses a classic blue “Leopard-era” vibe: gradients, depth, and a menu bar with a live clock. The app itself is contained inside a classic window chrome with traffic light buttons and a metallic title bar. This matters because it sets the tone before any interaction happens: when you see the UI, you immediately understand the reference and the mood.
                    </p>

                    <div class="image-block">
                        <img src="take%208%20images/movable-window.png" alt="Movable window interface">
                        <p class="image-caption">classic macos vibes with pinch-to-drag functionality</p>
                    </div>

                    <p>
                        Inside the window, the Photo Booth area leans into the classic dark stage-like interface. The camera feed becomes the center of gravity, and the controls feel like hardware: a large red capture button, an “Effects” mode, and a gallery strip. The UI is meant to invite playful experimentation—switching filters, taking photos, and collecting results—without breaking the illusion of a “real” desktop app.
                    </p>

                    <p>
                        High fidelity also meant sweating the micro-interactions: hover states, selection outlines, subtle shadows, and overlays (loading, countdown, flash). Those details are what turn an interface from “a demo” into something that feels complete.
                    </p>

                    <h3>Technical Implementation: Making the Prototype Actually Work</h3>

                    <p>
                        Even though the assignment emphasizes “look and feel,” I wanted this prototype to be genuinely functional because that changes how you design. When the camera is real, you have to handle permission errors. When filters run in real time, you have to think about performance. When ML detection controls the UI, you have to design around uncertainty.
                    </p>

                    <p>
                        The working system has three core technical pillars:
                    </p>

                    <p>
                        1. <strong>Live camera + rendering pipeline</strong><br>
                        The app requests webcam access, renders a mirrored “selfie” view, and continuously draws frames into canvas elements. From there, filters are applied as pixel operations so the preview feels immediate and tactile.
                    </p>

                    <p>
                        2. <strong>Filter browsing + viewing modes</strong><br>
                        Filters are shown as a grid of live previews (so the user can compare them quickly). You can select a filter and switch into a “single view” where the camera feed is larger and feels more like “the real shot.”
                    </p>

                    <div class="image-block">
                        <img src="take%208%20images/nailing=the=effects.png" alt="Effects panel implementation">
                        <p class="image-caption">nailing the effects panel: real-time filter previews</p>
                    </div>

                    <p>
                        3. <strong>Gallery + persistence</strong><br>
                        Captures aren’t just shown temporarily—they become artifacts: thumbnails appear in a filmstrip, and a dedicated gallery view lets you review, download, and delete. Images are stored using browser storage (IndexedDB), which makes the prototype behave more like an actual application rather than a one-off sketch.
                    </p>

                    <p>
                        On top of that, the embodied controls are implemented with ML models running in the browser:
                    </p>
                    
                    <ul>
                        <li><strong>Hand tracking (pinch)</strong> detects thumb/index landmarks, measures the distance between them, and uses stability rules (hysteresis + consecutive frames) so the pinch state doesn’t flicker.</li>
                        <li><strong>Face tracking (smile)</strong> computes mouth openness and width ratios and requires several consecutive frames before it triggers a capture, with a cooldown to prevent repeated shots.</li>
                    </ul>

                    <p>
                        The key point is that the “technical implementation” isn’t separate from the UX—it <em>is</em> the UX. The thresholds, frame requirements, cooldowns, and status indicators are all design decisions that make embodied input usable instead of frustrating.
                    </p>

                    <h3>Iteration Process: Building in Working Slices</h3>

                    <p>
                        I approached iteration by keeping the project <em>running</em> at every stage. Each iteration was a stable “slice” that added a capability without breaking the previous one. This was important because the assignment asks for a fully working version, and embodied/ML features can easily spiral if you don’t keep scope under control.
                    </p>
                    
                    <div class="image-block">
                        <img src="take%208%20images/issues-with-effects-panel.png" alt="Iteration challenges">
                        <p class="image-caption">working through layout challenges in the iteration process</p>
                    </div>

                    <p>
                        <strong>Iteration 1: Look & Feel (Visual Foundation)</strong><br>
                        I started by building the desktop environment and window system: the background, the menu bar, the time display, and the macOS-style window chrome. The goal was to establish a strong “product identity” early, so every later feature would live inside a cohesive world.
                    </p>

                    <p>
                        <strong>Iteration 2: Camera + Filters (Core Functionality)</strong><br>
                        Next I integrated the webcam and built real-time filter previews. This is where the prototype stopped being “a UI” and started being “a tool.” I focused on making the camera feel responsive and making effects exploration fast and fun.
                    </p>

                    <p>
                        <strong>Iteration 3: Embodied Control (ML Integration)</strong><br>
                        Then I added hand tracking and face tracking. This required iteration at a smaller scale: thresholds, stability rules, and UX feedback. The biggest design insight here was that embodied interactions need transparency—users need to see what the system thinks is happening. That’s why the status badges and smile indicator became essential, not decorative.
                    </p>

                    <p>
                        <strong>Iteration 4: Gallery + Polish (Completion Loop)</strong><br>
                        Finally, I added the gallery and “completion loop” features: filmstrip thumbnails, a gallery view, a preview modal, and actions like download/delete. This made the experience feel complete: the user can take photos, see results, keep what they like, and manage artifacts.
                    </p>

                    <h3>Scope Decisions: Limiting Features to Ship a Complete Experience</h3>

                    <p>
                        To keep the prototype fully working and high quality, I limited the embodied interaction set to <strong>two primary gestures</strong> (pinch + smile) and invested in making them reliable. Instead of adding more gesture types or extra modes that would introduce more edge cases, I prioritized stability and clarity.
                    </p>

                    <p>
                        This scope decision is directly aligned with the prompt: high fidelity is not the same as “lots of features.” High fidelity means coherence, polish, and a believable experience. A smaller set of interactions that works consistently is more valuable than a larger set that fails unpredictably.
                    </p>

                    <h3>What This Prototype Proves</h3>

                    <p>
                        This prototype demonstrates that “look and feel” can be deeply integrated with technical implementation and a real-world role. The nostalgic interface isn’t just decoration—it shapes how the user interprets the system. The ML controls aren’t just a trick—they reinforce the idea that interaction can be embodied, playful, and closer to human intention.
                    </p>

                    <p>
                        Most importantly, the final version is a working artifact: it captures photos, applies filters in real time, supports multiple input methods, and turns interaction into a small performance. That’s the point of a photo booth—and that’s why “body as controller” fits this role in life so well.
                    </p>
                    
                    <p>
                        <strong>
                            <a href="https://buribe13.github.io/refine-my-interface/" target="_blank" rel="noopener noreferrer">
                                Check out the final site →
                            </a>
                        </strong>
                    </p>
                </div>
            </article>
        </main>
    </div>
</body>
<script>
// Smooth scroll navigation and active state management
document.addEventListener('DOMContentLoaded', function() {
    const navLinks = document.querySelectorAll('.nav-link');
    const blogPosts = document.querySelectorAll('.blog-post');
    
    // Smooth scroll for navigation links
    navLinks.forEach(link => {
        link.addEventListener('click', function(e) {
            e.preventDefault();
            const targetId = this.getAttribute('href').substring(1);
            const targetElement = document.getElementById(targetId);
            
            if (targetElement) {
                targetElement.scrollIntoView({
                    behavior: 'smooth',
                    block: 'start'
                });
            }
        });
    });
    
    // Update active navigation based on scroll position
    function updateActiveNav() {
        let current = '';
        
        blogPosts.forEach(post => {
            const postTop = post.offsetTop;
            const postHeight = post.offsetHeight;
            const scrollPosition = window.scrollY + 150; // Increased offset for better detection
            
            if (scrollPosition >= postTop && scrollPosition < postTop + postHeight) {
                current = post.getAttribute('id');
            }
        });
        
        // Update active states with smooth transition
        navLinks.forEach(link => {
            const isActive = link.getAttribute('href') === '#' + current;
            
            if (isActive && !link.classList.contains('active')) {
                link.classList.add('active');
                link.style.opacity = '1';
                link.style.transform = 'translateX(5px)';
            } else if (!isActive && link.classList.contains('active')) {
                link.classList.remove('active');
                link.style.opacity = '0.4';
                link.style.transform = 'translateX(0px)';
            }
        });
    }
    
    // Throttled scroll event for better performance
    let ticking = false;
    function onScroll() {
        if (!ticking) {
            requestAnimationFrame(updateActiveNav);
            ticking = true;
            setTimeout(() => { ticking = false; }, 50); // Reduced throttle time
        }
    }
    
    // Initial call and scroll listener
    updateActiveNav();
    window.addEventListener('scroll', onScroll);
    
    // Handle hash fragments on page load
    function handleHashOnLoad() {
        const hash = window.location.hash;
        if (hash) {
            const targetId = hash.substring(1); // Remove the #
            const targetElement = document.getElementById(targetId);
            
            if (targetElement) {
                // Small delay to ensure page is fully loaded
                setTimeout(() => {
                    targetElement.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                    // Update active nav state
                    updateActiveNav();
                }, 100);
            }
        }
    }
    
    // Handle hash changes (when user clicks a link with hash)
    window.addEventListener('hashchange', function() {
        const hash = window.location.hash;
        if (hash) {
            const targetId = hash.substring(1);
            const targetElement = document.getElementById(targetId);
            
            if (targetElement) {
                targetElement.scrollIntoView({
                    behavior: 'smooth',
                    block: 'start'
                });
                updateActiveNav();
            }
        }
    });
    
    // Handle initial hash on page load
    handleHashOnLoad();
});
</script>
</html>

